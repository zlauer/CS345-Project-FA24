{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KbAg4-tSSxj"
   },
   "source": [
    "# CS 345 Final Project\n",
    "---\n",
    "Fynn Crossland, Zoe Lauer\n",
    "\n",
    "---\n",
    "\n",
    "We will be looking at the potability of water based on classifications of quality, and seeing if training models can accurately predict if water is potable or not. We will be using models we learned in class (SVM), and a model we found through research (Naive Bayes) and compare the results of the two models to see which is better.\n",
    "\n",
    "Data found at: https://www.kaggle.com/datasets/uom190346a/water-quality-and-potability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uiyD4HH5N077",
    "outputId": "87c2ab3d-6484-4bd1-dcd7-1e93ff0c8825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ph    Hardness        Solids  Chloramines     Sulfate  Conductivity  \\\n",
      "0       NaN  204.890455  20791.318981     7.300212  368.516441    564.308654   \n",
      "1  3.716080  129.422921  18630.057858     6.635246         NaN    592.885359   \n",
      "2  8.099124  224.236259  19909.541732     9.275884         NaN    418.606213   \n",
      "3  8.316766  214.373394  22018.417441     8.059332  356.886136    363.266516   \n",
      "4  9.092223  181.101509  17978.986339     6.546600  310.135738    398.410813   \n",
      "\n",
      "   Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
      "0       10.379783        86.990970   2.963135           0  \n",
      "1       15.180013        56.329076   4.500656           0  \n",
      "2       16.868637        66.420093   3.055934           0  \n",
      "3       18.436524       100.341674   4.628771           0  \n",
      "4       11.558279        31.997993   4.075075           0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.set_printoptions(precision=4)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Download latest version\n",
    "datas = pd.read_csv(\"water_potability.csv\")\n",
    "\n",
    "print(datas.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVno62w8jBc9"
   },
   "source": [
    "First we have to process the data. This being remove any rows that are mising values, and then split it into the labels and features maticies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpEQX0pCN2nI",
    "outputId": "23405b37-8d53-4417-cb76-0ba1bdb03d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2011,)\n",
      "(2011, 9)\n"
     ]
    }
   ],
   "source": [
    "datas = datas[~np.isnan(datas).any(axis = 1)]\n",
    "labels = datas.iloc[:,-1]\n",
    "features = datas.iloc[:,:-1]\n",
    "print(labels.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAZhYBUJjWtc"
   },
   "source": [
    "Now we are going to make the test and training sets that we are going to be using to train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hUodtAdJc61N"
   },
   "outputs": [],
   "source": [
    "## making of the train and test splits.\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFG6ypUUMpoy"
   },
   "source": [
    "First we are going to look at model that we learned in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtEFecLPvnE_"
   },
   "source": [
    "# Support Vector Machine(SVM)\n",
    "\n",
    "Within this model, I want to compare a linear kernal, a linear weighted SVM, as well as a nonlinear weighted SVM to compare the base accuracy.\n",
    "\n",
    "To begin, I'll be showcasing the SVM as a linear classifier with a C score of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hpt8TLEpMtoF",
    "outputId": "12c21034-89c2-478b-b6c0-7e8368fb02d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy for SVM:  0.5860927152317881\n",
      "Linear and unweighted classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74       354\n",
      "           1       0.00      0.00      0.00       250\n",
      "\n",
      "    accuracy                           0.59       604\n",
      "   macro avg       0.29      0.50      0.37       604\n",
      "weighted avg       0.34      0.59      0.43       604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "classifier = svm.SVC(kernel = 'linear', C=10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = (np.mean(y_test == y_pred))\n",
    "print(\"Base accuracy for SVM: \", accuracy)\n",
    "print(\"Linear and unweighted classification report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxGIPi0-ApSz"
   },
   "source": [
    "From here is an SVM with a linear kernel, balanced weight, and C score of 10.   What this is intended to illustrate is questioning the balance of the data set and understanding if there are further misclassifications.\n",
    "To begin with a linear kernel is intended to show off a difference in what happens whenever a balanced class_weight is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdP2AdciK-IH",
    "outputId": "b1c9d063-7969-442a-99ce-612c68c5bdfc"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "weighted_model_linear = SVC(kernel='linear', class_weight='balanced', C=10)\n",
    "weighted_model_linear.fit(X_train, y_train)\n",
    "y_weighted_linear = weighted_model_linear.predict(X_test)\n",
    "accuracy_weighted_linear = (np.mean(y_test == y_weighted_linear))\n",
    "print(\"Base accuracy for SVM: \", accuracy_weighted_linear)\n",
    "print(\"Linear with class weights:\")\n",
    "print(classification_report(y_test, y_weighted_linear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3Hi5N4fBbMw"
   },
   "source": [
    "Finally here is a non-linear, blanaced SVM classifier for consideration.  The intention for this is to further illustrate a comparison for the different forms of accuracy that may be present from SVM and what you can do with the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4snL756IMweY",
    "outputId": "33177fdb-e5d7-4195-afe6-4fbf3163bfa7"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "weighted_model = SVC(kernel='rbf', class_weight='balanced', C=10)\n",
    "weighted_model.fit(X_train, y_train)\n",
    "y_weighted = weighted_model.predict(X_test)\n",
    "accuracy_weighted = (np.mean(y_test == y_weighted))\n",
    "print(\"Base accuracy for SVM: \", accuracy_weighted)\n",
    "print(\"Nonlinear with class weights:\")\n",
    "print(classification_report(y_test, y_weighted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Py-DIfa58pdy"
   },
   "source": [
    "In looking at the base accuracy scores, the non-weighted SVM ended up having a fractionally better accuracy, and the weighted and linear SVM ended up having the worst base accuracy score.  In adding the **'class-weight'** parameter, it was meant to exhibit any form of false positives that were present in the data, as well as hopefully exemplify the minority class(1) better than the non-weighted and linear SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpNlbQTgMxaU"
   },
   "source": [
    "---\n",
    "Now to look at a model that we didn't experiement with in class, the Naive-Bayes model.\n",
    "---\n",
    "When looking at the Gaussian Naive Bayes model, it is looking at the data with the assumption that the data being looked at is of a normal distibution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0NR3KSJEkptV",
    "outputId": "59d9c2b8-1ca5-433f-f8b1-35aea041c679"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "clf_pred = clf.predict(X_test)\n",
    "g_test_accuracy = (np.mean(y_test == clf_pred))\n",
    "print(\"Base accuracy for Gaussian Naive Bayes: \", g_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FnnFRz11e0u"
   },
   "source": [
    "Now to look at a different kind of classifier, the Categorical Naive Bayes model, which looks at data that is categorically distributed. This is also true with our data which contains many categorical features that we are looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4zWHsrQM_NB",
    "outputId": "62a100ef-51c8-48c5-c8e1-6b24d9569741"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "clf = CategoricalNB()\n",
    "clf.fit(X_train, y_train)\n",
    "clf_pred = clf.predict(X_test)\n",
    "c_test_accuracy = (np.mean(y_test == clf_pred))\n",
    "print(\"Base accuracy for Categorical Naive Bayes: \", c_test_accuracy)\n",
    "diff = g_test_accuracy - c_test_accuracy\n",
    "print(\"\\nDifference between Gaussian model and Categorical model: \", diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9a1q1xy3WxB"
   },
   "source": [
    "When comparing the two base models, we can see that the Gaussian model is better when predicting with the data that we are using.\n",
    "\n",
    "However, is there a difference if we make changes to the Categorical model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "pnQh3xHmnw5M",
    "outputId": "b5cb145c-6024-4e7d-b493-24bb24488269"
   },
   "outputs": [],
   "source": [
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1., 10., 100., 1000.]\n",
    "test_accuracy = []\n",
    "for a in alphas:\n",
    "  clf = CategoricalNB(alpha = a)\n",
    "  clf.fit(X_train, y_train)\n",
    "  clf_pred = clf.predict(X_test)\n",
    "  accuracy = (np.mean(y_test == clf_pred))\n",
    "  test_accuracy.append(accuracy)\n",
    "print(\"Test accuracy for Categorical Naive Bayes: \", test_accuracy)\n",
    "print(\"The base alpha = 1.0 and the best alpha value = \", alphas[-2])\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.semilogx(alphas, test_accuracy, 'ob')\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Test Accuaracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vgENGHiG4pMd",
    "outputId": "5b77c569-7680-460d-afdc-6226a4be89c0"
   },
   "outputs": [],
   "source": [
    "best = np.max(test_accuracy)\n",
    "print(\"Best test accuracy: \", best,\"\\nCompared to the base Categorical model: \", best- c_test_accuracy)\n",
    "print(\"Difference between the Guassian model and best of the Categorical model with changed alpha scores: \", g_test_accuracy - best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd1dmuS6wEfG"
   },
   "source": [
    "The base Categorical Naive Bayes is only correct at predicting just over 50% of the time, while looking at using different alpha values, we find that the bigger the alpha, the better the prediction. However this can lead to overfitting, which is something that we do not want to happen with the training model.\n",
    "\n",
    "Even with the changes to the Categorical model, the Guassian model was better at predicting the data by about 3%, which isn't the biggest of differences however it is still enough to be a significant in deciding which model to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTKh8aabkaia"
   },
   "source": [
    "# What does this mean?\n",
    "\n",
    "\n",
    "---\n",
    "When looking at the two types of models, SVM and Naive Bayes, we saw that Naive Bayes Guassian model has the best accuracy at predicting the values of the training set. This being the difference of 62% accuracy of the Guassian model compared to the 59% accuracy of the best SVM model that we tested against.\n",
    "\n",
    "However, the range of accuracy that we found was 50-62%. This isn't that a large range, but isn't great when saying that any of the models we looked at predict accurately half the time. This can be a matter that the models that we used were not the best choice for the data that we were looking at. Yet, with the data we are looking at, knowing if water is potable is very important and predicting accurately if it is only half the time isn't ideal, but there is room to grow.\n",
    "\n",
    "---\n",
    "\n",
    "# The Future and Contributions\n",
    "\n",
    "If given more time, we would have explored more models with the data that we were looking at to try and find the best model.\n",
    "\n",
    "For this project, plans of action were done by both members. Fynn worked with the Naive Bayes model and all evaluation of it. Zoe worked with the SVM model and all evaluations of it. Conclusions and connections were done by both members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibmNWTiqo-Al"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
